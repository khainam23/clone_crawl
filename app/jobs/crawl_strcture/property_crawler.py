"""
Enhanced Property Crawler - Class ch√≠nh
"""

import asyncio
from typing import Dict, List, Any, Optional, Callable

from .property_extractor import PropertyExtractor
from .custom_rules import CustomExtractor
from .crawler_pool import CrawlerPool
from app.core.config import settings

class EnhancedPropertyCrawler:
    def __init__(self, custom_extractor_factory: Optional[Callable[[], CustomExtractor]] = None):
        """
        Initialize EnhancedPropertyCrawler
        
        Args:
            custom_extractor_factory: Optional factory function to create custom extractor
                                    If None, will use basic extractor
        """
        self.extractor = PropertyExtractor(custom_extractor_factory)
        self.custom_extractor_factory = custom_extractor_factory
        self.pool: Optional[CrawlerPool] = None

    async def _crawl_single_property(self, url: str, verbose: bool = True, pool: Optional[CrawlerPool] = None) -> Dict[str, Any]:
        """
        Private method ƒë·ªÉ crawl m·ªôt property
        
        Args:
            url: URL to crawl
            verbose: Whether to print progress messages (default: True)
            pool: Optional CrawlerPool instance to use
        """
        if verbose:
            print(f"üöÄ Crawling: {url}")
        
        crawler = None
        try:
            # N·∫øu c√≥ pool, l·∫•y crawler t·ª´ pool
            if pool:
                crawler = await pool.acquire()
                result = await self.extractor.extract_property_data(url, crawler=crawler)
            else:
                # Fallback: kh√¥ng d√πng pool
                result = await self.extractor.extract_property_data(url)
            
            # Tr·∫£ v·ªÅ tr·ª±c ti·∫øp property_data ƒë√£ ƒë∆∞·ª£c flatten trong extractor
            return result.get('property_data', result)
            
        except Exception as e:
            error_result = {
                'error': str(e),
            }
            if verbose:
                print(f"‚ùå Exception crawling {url}: {e}")
            return error_result
        finally:
            # Tr·∫£ crawler v·ªÅ pool n·∫øu ƒë√£ l·∫•y t·ª´ pool
            if pool and crawler:
                await pool.release(crawler)

    async def crawl_multiple_properties(
        self, 
        urls: List[str], 
        batch_size: int = 10,
        on_batch_complete: Optional[Callable[[List[Dict[str, Any]], int, int], Any]] = None,
        max_consecutive_failures: int = 30
    ) -> List[Dict[str, Any]]:
        """
        Crawl nhi·ªÅu properties v·ªõi batch processing v√† crawler pool ƒë·ªÉ tr√°nh timeout v√† overhead
        
        Args:
            urls: List of URLs to crawl
            batch_size: Number of URLs to crawl simultaneously (default: 10)
            on_batch_complete: Optional callback function ƒë∆∞·ª£c g·ªçi sau m·ªói batch
                             Nh·∫≠n params: (batch_results, batch_num, total_batches)
            max_consecutive_failures: Maximum consecutive failures before stopping (default: 30)
        """
        print(f"üèòÔ∏è Crawling {len(urls)} properties in batches of {batch_size}...")
        
        all_results = []
        consecutive_failures = 0
        
        # Kh·ªüi t·∫°o HTTP session pool v·ªõi k√≠ch th∆∞·ªõc b·∫±ng batch_size
        async with CrawlerPool(pool_size=batch_size, custom_extractor_factory=self.custom_extractor_factory) as pool:
            # Chia URLs th√†nh c√°c batches
            for i in range(0, len(urls), batch_size):
                batch_urls = urls[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                print(f"üì¶ Processing batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)...")
                
                # T·∫°o tasks cho batch hi·ªán t·∫°i v·ªõi pool
                tasks = [self._crawl_single_property(url, pool=pool) for url in batch_urls]
                
                # Ch·∫°y parallel v·ªõi error handling cho batch n√†y
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # X·ª≠ l√Ω exceptions cho batch v√† ƒë·∫øm consecutive failures
                processed_batch_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        processed_batch_results.append({
                            'error': str(result),
                            'url': batch_urls[j]
                        })
                        consecutive_failures += 1
                    else:
                        # Ki·ªÉm tra n·∫øu result c√≥ 'error' key (crawl failed)
                        if isinstance(result, dict) and 'error' in result:
                            consecutive_failures += 1
                        else:
                            # Reset counter ch·ªâ khi c√≥ success th·∫≠t s·ª±
                            if consecutive_failures > 0:
                                print(f"‚úÖ Success after {consecutive_failures} consecutive failures - counter reset")
                            consecutive_failures = 0
                        
                        processed_batch_results.append(result)
                    
                    # Ki·ªÉm tra threshold sau m·ªói result
                    if consecutive_failures >= max_consecutive_failures:
                        print(f"üõë Stopping crawl: {consecutive_failures} consecutive failures reached!")
                        print(f"‚ö†Ô∏è Crawled {len(all_results)} out of {len(urls)} URLs before stopping")
                        return all_results
                    elif consecutive_failures > 0 and consecutive_failures % 5 == 0:
                        print(f"‚ö†Ô∏è Warning: {consecutive_failures} consecutive failures (max: {max_consecutive_failures})")
                
                all_results.extend(processed_batch_results)
                
                # G·ªçi callback sau khi ho√†n th√†nh batch (n·∫øu c√≥)
                if on_batch_complete:
                    try:
                        await on_batch_complete(processed_batch_results, batch_num, total_batches)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error in batch callback: {e}")
                
                # Th√™m delay gi·ªØa c√°c batches ƒë·ªÉ tr√°nh qu√° t·∫£i server
                if i + batch_size < len(urls):  # Kh√¥ng delay sau batch cu·ªëi
                    print(f"‚è≥ Waiting {settings.CRAWLER_DELAY} seconds before next batch...")
                    await asyncio.sleep(settings.CRAWLER_DELAY)
        
        print(f"‚úÖ Completed crawling all {len(urls)} properties!")
        return all_results