"""
Enhanced Property Crawler - Class ch√≠nh
"""

import asyncio
from typing import Dict, List, Any, Optional, Callable

from .property_extractor import PropertyExtractor
from .custom_rules import CustomExtractor
from .crawler_pool import CrawlerPool
from app.core.config import settings

class EnhancedPropertyCrawler:
    def __init__(self, custom_extractor_factory: Optional[Callable[[], CustomExtractor]] = None):
        """
        Initialize EnhancedPropertyCrawler
        
        Args:
            custom_extractor_factory: Optional factory function to create custom extractor
                                    If None, will use basic extractor
        """
        self.extractor = PropertyExtractor(custom_extractor_factory)
        self.custom_extractor_factory = custom_extractor_factory
        self.pool: Optional[CrawlerPool] = None

    async def _crawl_single_property(self, url: str, verbose: bool = True, pool: Optional[CrawlerPool] = None) -> Dict[str, Any]:
        """
        Private method ƒë·ªÉ crawl m·ªôt property
        
        Args:
            url: URL to crawl
            verbose: Whether to print progress messages (default: True)
            pool: Optional CrawlerPool instance to use
        """
        if verbose:
            print(f"üöÄ Crawling: {url}")
        
        crawler = None
        try:
            # N·∫øu c√≥ pool, l·∫•y crawler t·ª´ pool
            if pool:
                crawler = await pool.acquire()
                result = await self.extractor.extract_property_data(url, crawler=crawler)
            else:
                # Fallback: kh√¥ng d√πng pool
                result = await self.extractor.extract_property_data(url)
            
            # Validate v√† t·∫°o PropertyModel
            property_model = self.extractor.validate_and_create_property_model(
                result['property_data']
            )
            
            # Convert v·ªÅ dict ƒë·ªÉ serialize JSON
            property_dict = property_model.dict(exclude_none=True)
            
            # Chuy·ªÉn ƒë·ªïi images th√†nh c√°c field ri√™ng bi·ªát
            if 'images' in property_dict and isinstance(property_dict['images'], list):
                images_list = property_dict.pop('images', [])
                for i, img in enumerate(images_list):
                    img_num = i + 1
                    if isinstance(img, dict):
                        if 'url' in img:
                            property_dict[f'image_url_{img_num}'] = img['url']
                        if 'category' in img:
                            property_dict[f'image_category_{img_num}'] = img['category']
                            
            # Chuy·ªÉn ƒë·ªïi station th√†nh c√°c field ri√™ng bi·ªát
            if 'stations' in property_dict and isinstance(property_dict['stations'], list):
                stations_list = property_dict.pop('stations', [])
                for i, station in enumerate(stations_list):
                    station_num = i + 1
                    if isinstance(station, dict):
                        if 'station_name' in station:
                            property_dict[f'station_name_{station_num}'] = station['station_name']
                        if 'train_line_name' in station:
                            property_dict[f'train_line_name_{station_num}'] = station['train_line_name']
                        if 'walk_time' in station:
                            property_dict[f'walk_time_{station_num}'] = station['walk_time']
            
            result['property_data'] = property_dict
            
            return result['property_data']
            
        except Exception as e:
            error_result = {
                'error': str(e),
            }
            if verbose:
                print(f"‚ùå Exception crawling {url}: {e}")
            return error_result
        finally:
            # Tr·∫£ crawler v·ªÅ pool n·∫øu ƒë√£ l·∫•y t·ª´ pool
            if pool and crawler:
                await pool.release(crawler)

    async def crawl_multiple_properties(
        self, 
        urls: List[str], 
        batch_size: int = 10,
        on_batch_complete: Optional[Callable[[List[Dict[str, Any]], int, int], Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Crawl nhi·ªÅu properties v·ªõi batch processing v√† crawler pool ƒë·ªÉ tr√°nh timeout v√† overhead
        
        Args:
            urls: List of URLs to crawl
            batch_size: Number of URLs to crawl simultaneously (default: 10)
            on_batch_complete: Optional callback function ƒë∆∞·ª£c g·ªçi sau m·ªói batch
                             Nh·∫≠n params: (batch_results, batch_num, total_batches)
        """
        print(f"üèòÔ∏è Crawling {len(urls)} properties in batches of {batch_size}...")
        
        all_results = []
        
        # Kh·ªüi t·∫°o crawler pool v·ªõi k√≠ch th∆∞·ªõc b·∫±ng batch_size
        async with CrawlerPool(pool_size=batch_size, custom_extractor_factory=self.custom_extractor_factory) as pool:
            # Chia URLs th√†nh c√°c batches
            for i in range(0, len(urls), batch_size):
                batch_urls = urls[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                print(f"üì¶ Processing batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)...")
                
                # T·∫°o tasks cho batch hi·ªán t·∫°i v·ªõi pool
                tasks = [self._crawl_single_property(url, pool=pool) for url in batch_urls]
                
                # Ch·∫°y parallel v·ªõi error handling cho batch n√†y
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # X·ª≠ l√Ω exceptions cho batch
                processed_batch_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        processed_batch_results.append({
                            'error': str(result),
                            'url': batch_urls[j]
                        })
                    else:
                        processed_batch_results.append(result)
                
                all_results.extend(processed_batch_results)
                
                # G·ªçi callback sau khi ho√†n th√†nh batch (n·∫øu c√≥)
                if on_batch_complete:
                    try:
                        await on_batch_complete(processed_batch_results, batch_num, total_batches)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error in batch callback: {e}")
                
                # Th√™m delay gi·ªØa c√°c batches ƒë·ªÉ tr√°nh qu√° t·∫£i server
                if i + batch_size < len(urls):  # Kh√¥ng delay sau batch cu·ªëi
                    print(f"‚è≥ Waiting {settings.CRAWLER_DELAY} seconds before next batch...")
                    await asyncio.sleep(settings.CRAWLER_DELAY)
        
        print(f"‚úÖ Completed crawling all {len(urls)} properties!")
        return all_results