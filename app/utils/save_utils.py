"""
File operation utility functions
"""
import logging, time
from typing import List, Dict, Any, Optional
import logging, time, json, os
from app.core.config import settings

from app.db.mongodb import get_collection

# Setup logger
logger = logging.getLogger(__name__)

class SaveUtils:
    """Utility functions cho file operations"""
    
    @staticmethod
    async def backup_db(collection_name: Optional[str] = "crawl_results") -> Optional[str]:
        """Backup MongoDB collection ra file JSON tr∆∞·ªõc khi th·ª±c hi·ªán thao t√°c"""
        try:
            collection = get_collection(collection_name)
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng documents
            count = await collection.count_documents({})
            
            if count == 0:
                logger.info(f"No data to backup in collection: {collection_name}")
                return None
            
            # L·∫•y t·∫•t c·∫£ documents t·ª´ collection
            cursor = collection.find({})
            documents = await cursor.to_list(length=None)
            
            # T·∫°o th∆∞ m·ª•c backup n·∫øu ch∆∞a t·ªìn t·∫°i
            backup_dir = "data/backups"
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            # T·∫°o t√™n file backup v·ªõi timestamp
            timestamp = int(time.time())
            filename = f"{backup_dir}/{collection_name}_backup_{timestamp}.json"
            
            # L∆∞u v√†o file JSON
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(documents, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"Backed up {count} documents from collection '{collection_name}' to: {filename}")
            
            return filename
            
        except Exception as e:
            logger.error(f"Error backing up MongoDB collection {collection_name}: {e}")
            print(f"‚ùå Error backing up MongoDB collection {collection_name}: {e}")
            return None
    
    @staticmethod
    async def clean_db(collection_name: Optional[str] = "crawl_results", auto_backup: bool = True):
        """L√†m r·ªóng database theo collection_name"""
        try:
            # T·ª± ƒë·ªông backup tr∆∞·ªõc khi x√≥a (n·∫øu auto_backup=True)
            if auto_backup:
                backup_file = await SaveUtils.backup_db(collection_name)
                if backup_file:
                    print(f"‚úÖ Backup completed before cleaning")
            
            collection = get_collection(collection_name)
            
            # X√≥a t·∫•t c·∫£ documents trong collection
            delete_result = await collection.delete_many({})
            deleted_count = delete_result.deleted_count
            
            logger.info(f"Cleaned {deleted_count} documents from MongoDB collection: {collection_name}")
            print(f"üßπ Cleaned {deleted_count} documents from MongoDB collection: {collection_name}")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"Error cleaning MongoDB collection {collection_name}: {e}")
            print(f"‚ùå Error cleaning MongoDB collection {collection_name}: {e}")
            return None
        
    @staticmethod
    async def filter_urls(urls: List[str], collection_name: Optional[str] = "crawl_results", id_fallback: Optional[int] = 0) -> tuple[List[str], List[int]]:
        """
        L·ªçc ra c√°c URLs ch∆∞a t·ªìn t·∫°i trong collection ho·∫∑c ƒë√£ qu√° h·∫°n c·∫≠p nh·∫≠t v√† l·∫•y danh s√°ch ID kh·∫£ d·ª•ng
        
        Ngo√†i ra ph·∫£i lo·∫°i b·ªè c√°c url trong collection kh√¥ng c√≤n t·ªìn t·∫°i khi trong c√°c url crawl v·ªÅ tr√°nh t√¨nh tr·∫°ng nh√† ƒë√£ ƒë∆∞·ª£c ƒë√≥ng
        
        Args:
            urls: Danh s√°ch URLs c·∫ßn ki·ªÉm tra
            collection_name: T√™n collection c·∫ßn ki·ªÉm tra
            id_fallback: ID m·∫∑c ƒë·ªãnh n·∫øu collection r·ªóng
            
        Returns:
            Tuple g·ªìm (danh s√°ch URLs ch∆∞a t·ªìn t·∫°i ho·∫∑c c·∫ßn c·∫≠p nh·∫≠t, danh s√°ch ID kh·∫£ d·ª•ng ƒë·ªÉ s·ª≠ d·ª•ng)
        """
        try:
            collection = get_collection(collection_name)
            
            # L·∫•y ID l·ªõn nh·∫•t hi·ªán t·∫°i trong collection
            max_id_doc = await collection.find_one(
                sort=[("_id", -1)],
                projection={"_id": 1}
            )
            # Convert string _id to int if needed
            if max_id_doc:
                max_id = int(max_id_doc["_id"]) if isinstance(max_id_doc["_id"], str) else max_id_doc["_id"]
            else:
                max_id = id_fallback
            
            if not urls:
                return [], []
            
            # T·∫°o index cho field 'link' n·∫øu ch∆∞a c√≥ (ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô query)
            try:
                await collection.create_index("link")
            except Exception as e:
                logger.warning(f"Index creation skipped or failed: {e}")
            
            # T√≠nh th·ªùi gian hi·ªán t·∫°i
            current_time = time.time()
            cutoff_time = current_time - settings.LAST_UPDATED
            
            # T√¨m c√°c URLs ƒë√£ t·ªìn t·∫°i v√† c√≤n m·ªõi (created_date > cutoff_time)
            # Ch·ªâ l·∫•y nh·ªØng URLs c√≤n fresh, kh√¥ng c·∫ßn c·∫≠p nh·∫≠t
            fresh_urls = await collection.distinct(
                "link", 
                {
                    "link": {"$in": urls},
                    "created_date": {"$gt": cutoff_time}
                }
            )
            fresh_urls_set = set(fresh_urls)
            
            # L·ªçc ra c√°c URLs ch∆∞a t·ªìn t·∫°i ho·∫∑c ƒë√£ qu√° h·∫°n
            new_urls = [url for url in urls if url not in fresh_urls_set]
            
            # X√≥a c√°c URLs trong collection kh√¥ng c√≤n t·ªìn t·∫°i trong danh s√°ch crawl
            # (Tr√°nh t√¨nh tr·∫°ng nh√† ƒë√£ ƒë∆∞·ª£c ƒë√≥ng nh∆∞ng v·∫´n c√≤n trong DB)
            urls_set = set(urls)
            delete_result = await collection.delete_many({
                "link": {"$nin": urls}  # URLs kh√¥ng c√≥ trong danh s√°ch crawl
            })
            deleted_count = delete_result.deleted_count
            
            if deleted_count > 0:
                logger.info(f"Removed {deleted_count} URLs no longer available in crawl results")
                print(f"üóëÔ∏è Removed {deleted_count} closed/unavailable listings from collection")
            
            # T√¨m c√°c ID gaps (ID b·ªã thi·∫øu) ƒë·ªÉ t√°i s·ª≠ d·ª•ng
            available_ids = []
            if len(new_urls) > 0:
                existing_ids = await collection.distinct("_id")
                existing_ids_set = set(int(i) for i in existing_ids)

                max_id = max(existing_ids_set) if existing_ids_set else id_fallback
                needed = len(new_urls)

                # T√¨m c√°c ID c√≤n tr·ªëng (gap)
                all_ids = set(range(id_fallback, max_id + 1))
                available_ids = list(all_ids - existing_ids_set)

                # C·∫Øt v·ª´a ƒë·ªß s·ªë l∆∞·ª£ng c·∫ßn thi·∫øt
                available_ids = available_ids[:needed]

                # N·∫øu ch∆∞a ƒë·ªß, th√™m c√°c ID m·ªõi
                if len(available_ids) < needed:
                    available_ids.extend(range(max_id + 1, max_id + 1 + (needed - len(available_ids))))
            
            gaps_count = len([id for id in available_ids if id <= max_id])
            new_ids_count = len([id for id in available_ids if id > max_id])
            
            print(f"üîç Filtered URLs: {len(urls)} total, {len(fresh_urls_set)} fresh, {len(new_urls)} to process | üÜî Available IDs: {gaps_count} gaps + {new_ids_count} new")
            
            return new_urls, available_ids
            
        except Exception as e:
            logger.error(f"Error filtering URLs from collection {collection_name}: {e}")
            print(f"‚ùå Error filtering URLs: {e}")
            # Tr·∫£ v·ªÅ t·∫•t c·∫£ URLs v√† danh s√°ch ID r·ªóng n·∫øu c√≥ l·ªói (fail-safe)
            return urls, []
    
    @staticmethod
    async def save_db_results(results: List[Dict[str, Any]], collection_name: Optional[str] = "crawl_results", available_ids: Optional[List[int]] = None) -> Optional[str]:
        """L∆∞u k·∫øt qu·∫£ v√†o MongoDB - Insert m·ªõi ho·∫∑c Update n·∫øu URL ƒë√£ t·ªìn t·∫°i"""        
        try:
            collection = get_collection(collection_name)
            
            inserted_count = 0
            updated_count = 0
            # S·ª≠ d·ª•ng danh s√°ch ID kh·∫£ d·ª•ng ho·∫∑c t·∫°o m·ªõi n·∫øu kh√¥ng c√≥
            if available_ids is None:
                available_ids = []
            
            id_index = 0  # Index ƒë·ªÉ l·∫•y ID t·ª´ available_ids
            
            required_field = [
                'link',
                'room_type', 
                'map_lat', 
                'map_lng', 
                'image_url_3', 
                'image_category_1',
                'image_category_2',
                'station_name_1',
                'floors', 
                'floor_no'
            ]
            
            for result in results:
                missing = [f for f in required_field if f not in result]
                if missing:
                    print(f"B·ªè qua v√¨ thi·∫øu c√°c field: {missing}")
                    continue
                
                if result['map_lat'] <= 0 or result['map_lng'] <= 0:
                    print(f"B·ªè qua v√¨ v·ªã tr√≠ kh√¥ng h·ª£p l·ªá: {result['link']}")
                    continue
                
                link = result.get("link")
                
                # Ki·ªÉm tra URL ƒë√£ t·ªìn t·∫°i ch∆∞a
                existing_doc = await collection.find_one({"link": link})
                
                if existing_doc:
                    # URL ƒë√£ t·ªìn t·∫°i ‚Üí UPDATE (gi·ªØ nguy√™n _id c≈©)
                    existing_id = str(existing_doc["_id"])
                    document = {
                        **result,
                        "created_date": time.time(),
                        "_id": existing_id   # Gi·ªØ nguy√™n _id c≈©
                    }
                    await collection.replace_one({"_id": existing_id}, document)
                    updated_count += 1
                else:
                    # URL m·ªõi ‚Üí INSERT v·ªõi _id t·ª´ available_ids
                    if id_index < len(available_ids):
                        new_id = available_ids[id_index]
                        id_index += 1
                    else:
                        # Fallback: n·∫øu h·∫øt available_ids, t√¨m max_id v√† tƒÉng d·∫ßn
                        max_id_doc = await collection.find_one(
                            sort=[("_id", -1)],
                            projection={"_id": 1}
                        )
                        if max_id_doc:
                            max_id = int(max_id_doc["_id"]) if isinstance(max_id_doc["_id"], str) else max_id_doc["_id"]
                            new_id = max_id + 1
                        else:
                            new_id = 1
                    
                    document = {
                        **result,
                        "created_date": time.time(),
                        "_id": str(new_id)  # Convert to string for MongoDB
                    }
                    await collection.insert_one(document)
                    inserted_count += 1
            
            total_count = inserted_count + updated_count
            if total_count > 0:
                logger.info(f"Saved {total_count} results to MongoDB collection '{collection_name}' (Inserted: {inserted_count}, Updated: {updated_count})")
                print(f"üíæ Saved {total_count} results to '{collection_name}' (‚ú® New: {inserted_count}, üîÑ Updated: {updated_count})")
                return f"{collection_name}"
            else:
                logger.warning("No results to save")
                print("‚ö†Ô∏è No results to save")
                return None
            
        except Exception as e:
            logger.error(f"Error saving to MongoDB collection {collection_name}: {e}")
            print(f"‚ùå Error saving to MongoDB: {e}")
            return None
        
    @staticmethod
    async def save_json_results(results: List[Dict[str, Any]], collection_name: Optional[str] = "crawl_results", _id: Optional[int] = 0) -> Optional[str]:
        try:
            # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a t·ªìn t·∫°i
            data_dir = "data"
            if not os.path.exists(data_dir):
                os.makedirs(data_dir)
            
            # T·∫°o t√™n file d·ª±a tr√™n collection_name v√† timestamp
            timestamp = int(time.time())
            filename = f"{data_dir}/{collection_name}_{timestamp}.json"
            
            # Chu·∫©n b·ªã documents ƒë·ªÉ l∆∞u
            documents = []
            for i, result in enumerate(results):
                document = {
                    **result,
                    "created_date": time.time(),
                    "_id": str(_id + i + 1)
                }
                documents.append(document)
            
            # L∆∞u v√†o file JSON
            if documents:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(documents, f, ensure_ascii=False, indent=2)
                
                saved_count = len(documents)
                logger.info(f"Saved {saved_count} results to JSON file: {filename}")
                print(f"üíæ Saved {saved_count} results to JSON file: {filename}")
                return filename
            else:
                logger.warning("No results to save")
                print("‚ö†Ô∏è No results to save")
                return None
                
        except Exception as e:
            logger.error(f"Error saving to JSON file: {e}")
            print(f"‚ùå Error saving to JSON file: {e}")
            return None
        